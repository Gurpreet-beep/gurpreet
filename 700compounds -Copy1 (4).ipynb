{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a4803-4c08-410d-9c27-d586d04758a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Molecular Descriptors Calculation using RDKit\n",
    "# ===========================================\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === 1. Load your Excel file ===\n",
    "input_file = \"pubchem_rest_smiles.xlsx\" # <-- your uploaded file\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# üîπ Change this to the column name that contains SMILES\n",
    "smiles_column = \"smiles\"  # <-- update if different\n",
    "\n",
    "# === 2. Prepare descriptor names ===\n",
    "descriptor_names = [desc_name for desc_name, _ in Descriptors._descList]\n",
    "\n",
    "# === 3. Compute descriptors ===\n",
    "descriptor_data = []\n",
    "\n",
    "print(\"Calculating molecular descriptors...\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    smi = str(row[smiles_column]).strip()\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        print(f\"Invalid SMILES: {smi}\")\n",
    "        continue\n",
    "\n",
    "    values = {}\n",
    "    for name, func in Descriptors._descList:\n",
    "        try:\n",
    "            values[name] = func(mol)\n",
    "        except Exception:\n",
    "            values[name] = None\n",
    "\n",
    "    values[\"smiles\"] = smi\n",
    "    descriptor_data.append(values)\n",
    "\n",
    "# === 4. Convert to DataFrame ===\n",
    "desc_df = pd.DataFrame(descriptor_data)\n",
    "\n",
    "# === 5. Save to Excel ===\n",
    "output_file = \"molecular_DEScriptors.xlsx\"\n",
    "desc_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done! Descriptors saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5500f-a7c6-449f-9276-2ee9993b345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(\"Molecular_Descriptors_Filtered700.xlsx\")\n",
    "y = pd.read_excel(\"experimental value .xlsx\")       # experimental values\n",
    "     # experimental values\n",
    "y = y.values.ravel()  # flatten if single column\n",
    "\n",
    "# Keep only numeric columns\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Remove zero-variance features\n",
    "zero_var_features = [col for col in df_numeric.columns if df_numeric[col].nunique() <= 1]\n",
    "df_numeric = df_numeric.drop(columns=zero_var_features)\n",
    "print(f\"Removed {len(zero_var_features)} zero-variance features.\")\n",
    "\n",
    "# Compute absolute correlation matrix\n",
    "corr_matrix = df_numeric.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Identify highly correlated features (|r| > 0.95)\n",
    "to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.95)]\n",
    "print(f\"Removed {len(to_drop)} highly correlated features.\")\n",
    "\n",
    "# Drop them from dataset\n",
    "df_reduced = df_numeric.drop(columns=to_drop)\n",
    "\n",
    "# Save to a new Excel file (choose your own filename)\n",
    "output_path = \"molecular_DEScriptors_filtered.xlsx\"\n",
    "df_reduced.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\nFiltered dataset saved as: {output_path}\")\n",
    "print(f\"Final dataset shape: {df_reduced.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032241af-c76d-43eb-a804-684a0d2b896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Load your dataset\n",
    "X = pd.read_excel(\"Molecular_Descriptors_Filtered.xlsx\")  # descriptor features\n",
    "y = pd.read_excel(\"experimental value .xlsx\")       # experimental values\n",
    "     # experimental values\n",
    "y = y.values.ravel()  # flatten if single column\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Step 3: RFE for top 20 descriptors\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rfe = RFE(estimator=model, n_features_to_select=20)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Print top 20 descriptors\n",
    "top20_descriptors = X.columns[rfe.support_]\n",
    "print(\"Top 20 descriptors selected by RFE:\")\n",
    "for i, feature in enumerate(top20_descriptors, start=1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# Create a DataFrame with feature names and their ranking\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Ranking': rfe.ranking_\n",
    "})\n",
    "\n",
    "# Mark top 20 features\n",
    "feature_ranking['Top20'] = feature_ranking['Ranking'].apply(lambda x: 'Yes' if x == 1 else 'No')\n",
    "\n",
    "# Save to a single CSV\n",
    "feature_ranking.to_csv('descriptors_rfe.csv', index=False)\n",
    "print(\"RFE results saved to 'descriptors_rfe.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83791ff-6f0a-4e54-9f03-b4c38ac95069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "import chardet\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Detect file encoding\n",
    "# -----------------------------\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(10000))  # read first 10k bytes\n",
    "    return result['encoding']\n",
    "\n",
    "# File paths\n",
    "descriptors_file = (\"Molecular_Descriptors_Filtered700.xlsx\")\n",
    "target_file = (\"experimental value .xlsx\")  \n",
    "\n",
    "# Detect encoding\n",
    "desc_encoding = detect_encoding(descriptors_file)\n",
    "target_encoding = detect_encoding(target_file)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Load CSVs with correct encoding\n",
    "# -----------------------------\n",
    "X = pd.read_excel(descriptors_file)\n",
    "y = pd.read_excel(target_file)\n",
    "y = y.values.ravel()  # flatten if single column\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: RFE to select top 20 descriptors\n",
    "# -----------------------------\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rfe = RFE(estimator=model, n_features_to_select=20)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "top20_descriptors = X.columns[rfe.support_]\n",
    "print(\"Top 20 descriptors selected by RFE:\")\n",
    "for i, feature in enumerate(top20_descriptors, start=1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Prepare data for EDA\n",
    "# -----------------------------\n",
    "eda_df = X[top20_descriptors].copy()\n",
    "eda_df['ExperimentalValue'] = y\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Summary statistics\n",
    "# -----------------------------\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(eda_df.describe())\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Correlation heatmap\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(12,10))\n",
    "corr_matrix = eda_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap of Top 20 Descriptors + Experimental Value\")\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Step 7: Distribution plots\n",
    "# -----------------------------\n",
    "# Distribution plots\n",
    "for col in top20_descriptors:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.histplot(data=eda_df, x=col, kde=True, bins=20)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plots vs target\n",
    "for col in top20_descriptors:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(data=eda_df, x=col, y='ExperimentalValue')\n",
    "    plt.title(f'{col} vs Experimental Value')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Experimental Value')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 9: Scatter plots vs Experimental Value\n",
    "# -----------------------------\n",
    "for col in top20_descriptors:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(x=eda_df[col], y=eda_df['ExperimentalValue'])\n",
    "    plt.title(f'{col} vs Experimental Value')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Experimental Value')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275275b-d50d-4b14-b04b-0f4699df4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use only top 20 descriptors\n",
    "X_top20 = X[top20_descriptors]\n",
    "\n",
    "# Split the dataset: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_top20, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Optional: check shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90da5ff-a02e-492d-9d93-7b8ab26b994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you already have:\n",
    "# X -> your full feature DataFrame\n",
    "# y -> your target variable\n",
    "# Top20_descriptors -> list of the 20 selected feature names\n",
    "\n",
    "# Select only top 20 descriptors\n",
    "\n",
    "X_top20 = X[top20_descriptors]\n",
    "\n",
    "# Split dataset (optional step)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_top20, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Combine X and y to save together (optional)\n",
    "train_data = X_train.copy()\n",
    "train_data[\"pEC50\"] = y_train\n",
    "\n",
    "test_data = X_test.copy()\n",
    "test_data[\"PEC50\"] = y_test\n",
    "\n",
    "# Save to Excel file\n",
    "with pd.ExcelWriter(\"Top20_descriptors_data.xlsx\") as writer:\n",
    "    train_data.to_excel(writer, sheet_name=\"Train Data\", index=False)\n",
    "    test_data.to_excel(writer, sheet_name=\"Test Data\", index=False)\n",
    "\n",
    "print(\"‚úÖ Excel file 'Top20_descriptors_data.xlsx' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac1530-c158-4f1d-9a2d-a8049d383079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Initialize and train Random Forest\n",
    "# -----------------------------\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Make predictions\n",
    "# -----------------------------\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Evaluate model\n",
    "# -----------------------------\n",
    "def evaluate_model(y_true, y_pred, dataset_name=\"Dataset\"):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"--- {dataset_name} Evaluation ---\")\n",
    "    print(f\"R¬≤   : {r2:.4f}\")\n",
    "    print(f\"MAE  : {mae:.4f}\")\n",
    "    print(f\"MSE  : {mse:.4f}\")\n",
    "    print(f\"RMSE : {rmse:.4f}\\n\")\n",
    "\n",
    "# Evaluate on training data\n",
    "evaluate_model(y_train, y_train_pred, \"Training Set\")\n",
    "\n",
    "\n",
    "# Evaluate on testing data\n",
    "evaluate_model(y_test, y_test_pred, \"Test Set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959395bf-b052-42a9-b174-a915de50a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "x_categorical = df.select_dtypes(include=['object']).apply(label_encoder.fit_transform)\n",
    "x_numerical = df.select_dtypes(exclude=['object']).values\n",
    "x = pd.concat([pd.DataFrame(x_numerical), x_categorical], axis=1).values\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=10, random_state=0, oob_score=True)\n",
    "\n",
    "regressor.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c898e51-7d3a-4ddc-a3e1-a75e609d5e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Try different numbers of top features\n",
    "for k in [10, 15, 20, 25, 30]:\n",
    "    selector = RFE(model, n_features_to_select=k)\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    scores = cross_val_score(model, X.iloc[:, selector.support_], y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    rmse = -np.mean(scores)\n",
    "    print(f\"Top {k} features ‚Üí RMSE: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4301a-ce96-4589-9960-499b33485580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Scatter plot: Predicted vs Actual\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=y_test, y=y_test_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)  # diagonal line\n",
    "plt.xlabel(\"Actual Experimental Values\")\n",
    "plt.ylabel(\"Predicted Experimental Values\")\n",
    "plt.title(\"Random Forest: Predicted vs Actual (Test Set)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ca62d-fa32-4a3b-bed7-43c739a09b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Example: Replace with your data loading step\n",
    "# X, y = ...\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define grid of hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000, 1500],\n",
    "    'max_depth': [ 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the model with best hyperparameters\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'R¬≤: {r2:.4f}')\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'MSE: {mse:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf482bcc-2f8d-4b1b-969b-caa8bca3af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb3d78-4d3c-411a-aa61-058df2bc667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=15,\n",
    "    num_leaves=100,\n",
    "    min_data_in_leaf=10,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30847b4-a3c1-43df-acde-7f0435996e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl xlrd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb712964-0a8e-412c-8f63-ef2fecbda21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# 1Ô∏è‚É£ Load your data\n",
    "df_x = pd.read_excel(\"C:/Users/gkjhe/Downloads/700 compunds of pesticides/Top20_descriptors_data.xlsx\")\n",
    "df_y = pd.read_excel(\"C:/Users/gkjhe/Downloads/700 compunds of pesticides/experimental value .xlsx\",engine=\"openpyxl\")\n",
    "print(df_x.head())\n",
    "print(df_y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1a846-8eea-4e33-be1e-64576c840768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# 1Ô∏è‚É£ Load your data\n",
    "df_x = pd.read_excel(\"C:/Users/gkjhe/Downloads/700 compunds of pesticides/8 molecualr desriptors.xlsx\")\n",
    "df_y = pd.read_excel(\"C:/Users/gkjhe/Downloads/700 compunds of pesticides/experimental value .xlsx\",engine=\"openpyxl\")\n",
    "\n",
    "# Ensure y is a 1D array (LightGBM expects this)\n",
    "y = y.squeeze()  # Converts DataFrame to Series if needed\n",
    "\n",
    "# 2Ô∏è‚É£ Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Base model\n",
    "model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# 4Ô∏è‚É£ Parameter grid\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 63, 127, 255],\n",
    "    'max_depth': [10, 15, 20, -1],\n",
    "    'learning_rate': [0.01, 0.03, 0.05],\n",
    "    'n_estimators': [1000, 2000, 3000],\n",
    "    'min_data_in_leaf': [10, 20, 30, 50],\n",
    "    'feature_fraction': [0.7, 0.8, 0.9],\n",
    "    'bagging_fraction': [0.7, 0.8, 0.9],\n",
    "    'subsample_freq': [1, 5, 10],\n",
    "    'reg_alpha': [0.0, 0.1, 0.5],\n",
    "    'reg_lambda': [0.0, 0.5, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# 5Ô∏è‚É£ Randomized Search setup\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,  # Increase for better tuning\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 6Ô∏è‚É£ Run search\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# 7Ô∏è‚É£ Evaluate best model\n",
    "print(\"‚úÖ Best Parameters:\", search.best_params_)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nüìä Model Performance on Test Set:\")\n",
    "print(\"R¬≤:\", r2_score(y_test, y_pred))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=TRUE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b199997-f1d8-4d58-b740-b60527331a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# 2. Load your data (X = descriptors, y = activities)\n",
    "X = pd.read_excel(\"C:/Users/gkjhe/Downloads/700 compunds of pesticides/8 molecualr desriptors.xlsx\")\n",
    "y = pd.read_excel(\"C:/Users/gkjhe/Downloads/700 compunds of pesticides/experimental value .xlsx\")\n",
    "\n",
    "# 3. Define the base model (Random Forest)\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4. Wrap it in Multi-Target Regressor (for MTR)\n",
    "mtr_model = MultiOutputRegressor(rf)\n",
    "\n",
    "# 5. Define k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "# 6. Perform cross-validation manually\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    mtr_model.fit(X_train, y_train)\n",
    "    y_pred = mtr_model.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    r2_scores.append(r2)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "# 7. Display results\n",
    "print(\"Average R¬≤:\", np.mean(r2_scores))\n",
    "print(\"Average RMSE:\", np.mean(rmse_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1552b4-810a-4069-8119-af7fe68bd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "üìÑ QSAR Model Summary\n",
    "---------------------\n",
    "Algorithm: Random Forest (Multi-Target)\n",
    "Cross-Validation: 5-Fold\n",
    "Average R¬≤: {0.682:.3f}\n",
    "Average RMSE: {1.115:.3f}\n",
    "\n",
    "Interpretation:\n",
    "- The model captures about 68% of variance across targets.\n",
    "- Typical prediction error is ~1.1 units.\n",
    "- Performance indicates a robust, generalizable QSAR model suitable for virtual screening or lead prioritization.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a88d2e-5cd0-4164-9fdc-a9c23455599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost lightgbm -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbe4de-d3ae-4cfd-809a-31bbadd99ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "# 2. Load your data (X = descriptors, y = activities)\n",
    "X = pd.read_excel(\"C:/Users/gkjhe/Downloads/700 compunds of pesticides/8 molecualr desriptors.xlsx\")\n",
    "y = pd.read_excel(\"C:/Users/gkjhe/Downloads/700 compunds of pesticides/experimental value .xlsx\")\n",
    "\n",
    "target_names = y.columns.tolist()\n",
    "\n",
    "# 3. Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=30,\n",
    "        min_samples_split=3,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=50,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "# 4. Cross-validation setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 5. Evaluation loop\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nüî¨ Evaluating {model_name}...\")\n",
    "    \n",
    "    mtr = MultiOutputRegressor(model)\n",
    "    r2_scores, rmse_scores = [], []\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        mtr.fit(X_train, y_train)\n",
    "        y_pred = mtr.predict(X_test)\n",
    "        \n",
    "        r2 = r2_score(y_test, y_pred, multioutput='uniform_average')\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        r2_scores.append(r2)\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    avg_r2 = np.mean(r2_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Avg R2': avg_r2,\n",
    "        'Avg RMSE': avg_rmse\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ {model_name} ‚Üí Avg R¬≤: {avg_r2:.3f} | Avg RMSE: {avg_rmse:.3f}\")\n",
    "\n",
    "# 6. Compare all models\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Model Comparison Summary:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa5afe-2b4d-48cd-a2a0-3e7088e39d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b438de6c-2016-4cca-8414-8db7c5281880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVR model\n",
    "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"SVR R2: {r2:.4f}\")\n",
    "print(f\"SVR RMSE: {rmse:.4f}\")\n",
    "\n",
    "# --- Plot 1: Actual vs Predicted ---\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title(f'SVR Predicted vs Actual\\nR¬≤={r2:.3f}, RMSE={rmse:.3f}')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Residual Plot (optional) ---\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "plt.axhline(0, color='r', linestyle='--')\n",
    "plt.title(\"SVR Residual Plot\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41be613-8b4a-435d-9ac8-8ce405ae6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR(kernel='rbf'))\n",
    "])\n",
    "\n",
    "svr_pipeline.fit(X_train, y_train)\n",
    "y_pred = svr_pipeline.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Scaled SVR R2: {r2:.4f}, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51606fa-ed74-42a7-84d0-3b9e0a20c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Your existing code\n",
    "# svr_pipeline.fit(X_train, y_train)\n",
    "# y_pred = svr_pipeline.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6, edgecolor='k')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title(f'SVR Predictions vs Actuals\\nR¬≤ = {r2:.3f}, RMSE = {rmse:.3f}')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bbf09-14cd-4ee5-a995-aabef5bf3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'svr__C': [0.1, 1, 10, 100],\n",
    "    'svr__gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "    'svr__epsilon': [0.01, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svr_pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV R2:\", grid_search.best_score_)\n",
    "\n",
    "best_svr = grid_search.best_estimator_\n",
    "y_pred = best_svr.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Tuned SVR R2: {r2:.4f}, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd30bc5-4b29-4d9c-8ecd-91be242831f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Sample smaller subsets for speed\n",
    "X_train_sample = shap.sample(X_train, 100, random_state=42)\n",
    "X_test_sample = shap.sample(X_test, 100, random_state=42)\n",
    "\n",
    "# Define a wrapper that applies scaling + prediction\n",
    "def pipeline_predict(X):\n",
    "    return best_svr.predict(X)\n",
    "\n",
    "# Use the preprocessed data (SHAP will feed already scaled values if we handle scaling inside the pipeline)\n",
    "explainer = shap.KernelExplainer(pipeline_predict, X_train_sample)\n",
    "shap_values = explainer.shap_values(X_test_sample, nsamples=100)\n",
    "\n",
    "# Plot global summary\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480813e4-f0c9-411c-baf7-3996e97da22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# --- 1Ô∏è‚É£  Prepare sampled data for SHAP (small subset = faster)\n",
    "X_train_sample = shap.sample(X_train, 100, random_state=42)\n",
    "X_test_sample = shap.sample(X_test, 100, random_state=42)\n",
    "\n",
    "# --- 2Ô∏è‚É£  Extract the scaler and model from the pipeline\n",
    "scaler = best_svr.named_steps['scaler']\n",
    "svr_model = best_svr.named_steps['svr']\n",
    "\n",
    "# --- 3Ô∏è‚É£  Scale data before passing to SHAP\n",
    "X_train_scaled = scaler.transform(X_train_sample)\n",
    "X_test_scaled = scaler.transform(X_test_sample)\n",
    "\n",
    "# --- 4Ô∏è‚É£  Define a simple prediction wrapper for the SVR model\n",
    "def svr_predict(X_scaled):\n",
    "    return svr_model.predict(X_scaled)\n",
    "\n",
    "# --- 5Ô∏è‚É£  Initialize SHAP explainer\n",
    "explainer = shap.KernelExplainer(svr_predict, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_test_scaled, nsamples=100)\n",
    "\n",
    "# --- 6Ô∏è‚É£  Plot global summary\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10ca43-0ff7-4ab9-98dd-2e8ff32c6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ Train/test split (replace X, y with your data)\n",
    "# ============================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ SVR pipeline with scaling + hyperparameter tuning\n",
    "# ============================================================\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svr__C': [1, 10, 100],\n",
    "    'svr__gamma': ['scale', 0.1, 1],\n",
    "    'svr__epsilon': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_svr = grid.best_estimator_\n",
    "\n",
    "y_pred = best_svr.predict(X_test)\n",
    "print(f\"Best SVR Params: {grid.best_params_}\")\n",
    "print(f\"R¬≤: {r2_score(y_test, y_pred):.3f}, RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ SHAP INTERPRETABILITY (bar + summary)\n",
    "# ============================================================\n",
    "scaler = best_svr.named_steps['scaler']\n",
    "svr_model = best_svr.named_steps['svr']\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "explainer = shap.KernelExplainer(svr_model.predict, shap.sample(X_train_scaled, 100, random_state=42))\n",
    "shap_values = explainer.shap_values(shap.sample(X_test_scaled, 100, random_state=42))\n",
    "\n",
    "# --- Summary Plot (beeswarm) ---\n",
    "shap.summary_plot(shap_values, shap.sample(X_test_scaled, 100, random_state=42), feature_names=X.columns)\n",
    "\n",
    "# --- Bar Plot (mean absolute SHAP values) ---\n",
    "shap.summary_plot(shap_values, shap.sample(X_test_scaled, 100, random_state=42), feature_names=X.columns, plot_type=\"bar\")\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ APPLICABILITY DOMAIN (Mahalanobis distance + leverage plot)\n",
    "# ============================================================\n",
    "# Compute Mahalanobis distance for AD\n",
    "cov = np.cov(X_train_scaled, rowvar=False)\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "mean_vec = X_train_scaled.mean(axis=0)\n",
    "\n",
    "def mahalanobis_dist(X_scaled):\n",
    "    return np.array([distance.mahalanobis(x, mean_vec, inv_cov) for x in X_scaled])\n",
    "\n",
    "train_maha = mahalanobis_dist(X_train_scaled)\n",
    "test_maha = mahalanobis_dist(X_test_scaled)\n",
    "threshold = np.percentile(train_maha, 95)  # AD threshold\n",
    "\n",
    "# --- AD Histogram ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(test_maha, bins=30, alpha=0.7, label='Test')\n",
    "plt.axvline(threshold, color='r', linestyle='--', label='95% AD threshold')\n",
    "plt.xlabel(\"Mahalanobis Distance\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Applicability Domain (Mahalanobis Distance)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Leverage Plot (distance vs residuals) ---\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(test_maha, residuals, alpha=0.7)\n",
    "plt.axvline(threshold, color='r', linestyle='--', label='AD limit (95%)')\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.xlabel(\"Mahalanobis Distance\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Applicability Domain - Leverage Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f731e-30b8-44c8-b82f-a48f332d655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor   # make sure lightgbm is installed\n",
    "\n",
    "# --- Cross-Validation Setup ---\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42),\n",
    "    \"SVM\": SVR(),\n",
    "    \"XGBoost\": XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"LightGBM\": LGBMRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# --- Evaluate each model ---\n",
    "for name, model in models.items():\n",
    "    y_pred = cross_val_predict(model, X, y, cv=kf)\n",
    "    Q2 = r2_score(y, y_pred)\n",
    "    print(f\"{name} Q¬≤ = {Q2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8a85c-0918-4e38-820a-df8316f12a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict, KFold, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Example: Split dataset\n",
    "# -------------------------\n",
    "# X, y are your features and target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# SVM pipeline with scaling\n",
    "# -------------------------\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVR(kernel='rbf', C=10, gamma=0.01, epsilon=0.1))  # example params\n",
    "])\n",
    "\n",
    "# -------------------------\n",
    "# Cross-validated predictions (for Q¬≤)\n",
    "# -------------------------\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_cv_pred = cross_val_predict(model, X_train, y_train, cv=cv)\n",
    "\n",
    "# -------------------------\n",
    "# Fit model on full training set\n",
    "# -------------------------\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# -------------------------\n",
    "# Function to calculate QSAR metrics\n",
    "# -------------------------\n",
    "def qsar_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    q2 = r2  # For CV predictions, Q¬≤ is essentially R¬≤ of CV\n",
    "    return {'R¬≤': r2, 'Q¬≤': q2, 'RMSE': rmse, 'MAE': mae}\n",
    "\n",
    "# -------------------------\n",
    "# Metrics for training (CV) and test\n",
    "# -------------------------\n",
    "train_metrics = qsar_metrics(y_train, y_cv_pred)\n",
    "test_metrics = qsar_metrics(y_test, y_test_pred)\n",
    "\n",
    "print(\"Training (CV) metrics:\", train_metrics)\n",
    "print(\"Test metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fd8ca-5da1-4299-b3fc-23d43a7507ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf      = results[\"RandomForest\"][\"test_pred\"]\n",
    "y_pred_svm     = results[\"SVR\"][\"test_pred\"]\n",
    "y_pred_xgb     = results[\"XGBoost\"][\"test_pred\"]\n",
    "y_pred_lgbm    = results[\"LightGBM\"][\"test_pred\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e8afd6-79d2-45fd-aad5-70e082afeece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "predictions = {\n",
    "    \"Random Forest\": y_pred_rf,\n",
    "    \"SVM\": y_pred_svm,\n",
    "    \"XGBoost\": y_pred_xgb,\n",
    "    \"LightGBM\": y_pred_lgbm\n",
    "}\n",
    "\n",
    "y_test_plot = np.ravel(y_test)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "for ax, (name, y_pred) in zip(axes.flatten(), predictions.items()):\n",
    "    y_pred = np.ravel(y_pred)\n",
    "\n",
    "    ax.scatter(y_test_plot, y_pred, alpha=0.6)\n",
    "\n",
    "    # Identity line\n",
    "    mn = min(y_test_plot.min(), y_pred.min())\n",
    "    mx = max(y_test_plot.max(), y_pred.max())\n",
    "    ax.plot([mn, mx], [mn, mx], 'r--')\n",
    "\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Actual\")\n",
    "    ax.set_ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28771762-b377-4bef-979c-debdb9bac15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
